# using DQN to play tetris
# This coursera course was a great example that I used to 
# implement this agent: https://github.com/udacity/deep-reinforcement-learning/blob/master/dqn/solution/dqn_agent.py
# state = board given by
from copy import copy , deepcopy
import torch

import numpy as np
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class DQN_Model(nn.Module):
    def __init__(self,input_size):
        super(DQN_Model, self).__init__()
        print("INPUT SIZE:", input_size)
        self.dense1 = nn.Linear(input_size,512)
        self.dense2 = nn.Linear(512,1024)
        self.dense3 = nn.Linear(1024,2048)
        self.dense4 = nn.Linear(2048,1024)
        self.dense5 = nn.Linear(1024,512)
        self.dense6 = nn.Linear(512,4)

    def forward(self, x):
        print("given this shape: ",x.shape)
        x = F.relu(self.dense1(x))
        x = F.relu(self.dense2(x))
        x = F.relu(self.dense3(x))
        x = F.relu(self.dense4(x))
        x = F.relu(self.dense5(x))
        output = F.relu(self.dense6(x))
        return output

# possible outputs 
# move left, move right, rotate, do nothing
# maybe add the shoot down option
class DQN_Agent():

    def __init__(self,input_size):
        # gamma is the decay rate
        # or the amount that we don't want reward to propigate
        self.gamma = .9
        self.prob_random = .1
        # define the layers
        # will come in as flattend rep of the board game
        self.model = DQN_Model(input_size)

        self.optimizer = optim.Adam(self.model.parameters(),lr=1e-3)

        self.data = []

        self.data_max = 100000

    def add_to_data(self,zipped_info):
        # check to make sure that data has room
        zipped_info = list(zipped_info)
        room_left = self.data_max - len(self.data) - len(list(zipped_info))
        if(room_left < 0):
            # remove from the front
            del self.data[0:abs(room_left)]

        # add the info in
        self.data.take_in_data(zipped_info)
        

    def take_in_data(self,data):

        # take in the data
        # flatten the observations
        # put everything into numpy arrays
        obs,action,new_obs,reward,done = zip(*data)
        obs = torch.tensor(obs).flatten(1)
        obs = list(obs)
        new_obs = torch.tensor(new_obs).flatten(1)
        new_obs =list(new_obs)
        
        print(type(obs),type(new_obs))

        self.data.extend(zip(obs,action,new_obs,reward,done))


    def train(self):
        # this should be fun
        # we want our network to predict the rewards given that input

        # loss we want to maximize is 
        # reward + gamma*max_a'Q(s',a',parameters) - Q(s,a,parameters) 
        # the Q(s,a,parameters) is just the current value
        # so we are going to say that 
        # max_a'Q(s',a',parameters) is just how much reward we expect to get in this state, taking this action
        # Luckily our model takes in a state and produces the amount of reward we expect if we took each action

        # idea written in a formula
        # Update q values
        # Q[state, action] = Q[state, action] + lr * (reward + gamma * np.max(Q[new_state, :]) â€” Q[state, action])

        # how do we formulate that as a deep learning question
        # we want 
        obs,actions,new_obs,rewards,dones = zip(*self.data)
        obs = torch.tensor(obs)
        new_obs = torch.tensor(new_obs)
        print(type(obs),type(new_obs))
        # all of these are numpy arrays
        # if something looks weird it could be because
        # we are vectorizing this calculation
        model_value_of_next_state = self.model.forward(new_obs).max(1).detach()
        model_value_of_next_state = np.expand_dims(model_value_of_next_state,1) # turn it from [1,2,3] to [[1],[2],[3]] 

        target_for_this_state = rewards + self.gamma*model_value_of_next_state*(1-dones)

        # TODO Explain Gather
        model_current_estimate = self.model.forward(obs).gather(1,actions)
        
        loss = F.mse_loss(model_current_estimate,target_for_this_state)

        self.optimzier.zero_grad()
        # back up those gradients
        loss.backward()
        self.optimizer.step()








        for i in range(len(backed_up_reward)):
            if(len(obs[i]) != 0 and not done[i]):
                backed_up_reward[i] += self.gamma*best_next_actions[i] + backed_up_reward[i+1]
            else:
                # no observation that makes any sense
                backed_up_reward[i] = reward[i]

    def predict(self,obs):
        if(random.random() < self.prob_random):
            return random.randint(0,3)
        
        obs = torch.tensor(obs).flatten().float()

        answer = self.model.forward(obs).detach()
        return np.argmax(answer)





        


